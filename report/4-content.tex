\chapter{Algorithm, Formalisation, Benchmark}\label{chap:content}

\section{Algorithm 1}
% Dr. Gregor Behnke already implemented a technique for 'this' available in the PANDA-3 planner. It was used to create many IPC TO domains, though it was never published, i.e., neither described nor properties like preserving of solutions were investigated.
Domain $(F, T_P, T_C, \delta, M)$ and Problem = $(T_I, S_0, TN_G)$
 
\begin{algorithm}
	% \SetAlgoLined
	\SetKwProg{Fn}{Function}{}{end}
	\Fn{GetPreEff($F, T_P, T_C, \delta, M, visited$)}{ %\TitleOfAlgo{GetPreEff}  	%\ProcNameSty{GetPreEff} \;
	%\KwData{$F, T_P, T_C, \delta, M, visited$}
	%\KwResult{$\PreS, \AddS, \DelS$}
	function\
	\For {c $\in$ $T_C$} {
		\If {c $\notin$ visited} {
			\For {t $\in$ $\{ t \vert  m \in M \land m(c) = tn\}$ } { % each sub-task of each method
				\For {a $\in$ F} {
					\eIf {t $\in T_P$} {
						$\PreS[v] = \PreS[v] \land Pre(t)$
						$\AddS[v] = \AddS[v] \land Add(t)$
						$\DelS[v] = \DelS[v] \land Del(t)$
					}{
						visited.add(c) \;
						GetPreEff(F, $T_P$, $T_C$, $\delta$, M, visited)\;
					}
				}
			}
		}
	}
	\Return {$\PreS, \AddS, \DelS$}\;
	\caption{Calculate all possible preconditions and effects for compound tasks}
}
\end{algorithm}

	
\begin{algorithm}[H]
	\KwData{$(F, T_P, T_C, \delta, M), \PreS, \AddS, \DelS$}
	\KwResult{$(F, T_P, T_C, \delta, M)$}

	\For {m $\in$ M}{
		$NS_m$  $\gets \emptyset$\;
		\For{a $\in$ F}{
			$NS_m$  = $\{ (t, t', 1) |  (a \in \AddS(t) \land a \in \PreS(t') \land t, t' \in tasks(m) ) \}$ $\cup$ $NS_m$  \;
		 	$NS_m$  = $\{ (t', t, 1) |  (a \in \AddS(t) \land a \in \DelS(t') \land t, t' \in tasks(m) ) \}$ $\cup$ $NS_m$  \;
			$NS_m$  =  $\{ (t', t, 1) |  (a \in \DelS(t) \land a \in \PreS(t') \land t, t' \in tasks(m) ) \}$ $\cup$ $NS_m$  \;
			$NS_m$  =  $\{ (t, t', 1) |  (a \in \DelS(t) \land a \in \AddS(t') \land t, t' \in tasks(m) ) \}$ $\cup$ $NS_m$  \;
		}{\tiny }
		directed graph G =(tasks(m),E)\;
		E $\gets \emptyset$ \;
		\For {ordering $\in \prec$} {
			E = E $\cup$ (ordering, 0)  
		} 
		\For {(t, t', w) $\in NS_m$}{
			E = E $\cup$ ((t,t'), w)  
		}       
	    BackEdges $\gets$ DFS(G) \;
	    \For{e $\in$ BackEdges}{
	     	\If {weight(e) = 1}  {E = E $\setminus$ e} 
	   		\If {weight(e) = 0} {
	   			A, B = e \; 
	   		  	Path = Dijkstra(B, A)  \;
	     		randEdge = RANDOM( \{e $\vert$ weight(e) = 0 $\land$ e $\in Path$ \} ) \;
	     		E = E $\setminus$ randEdge
	     	}
     	}
     	$\prec'$ = TopologicalSort(G) \;
     	$m' = (tasks(m), \prec', \alpha(t))$ \;
     	$M' = m' \cup M'$ \;
	}
	Return new domain = $(F, T_P, T_C, \delta, M')$
	\caption{Calculation of linearized methods}
\end{algorithm}

 
\begin{enumerate}
	\item GetPreEff (time)= NumMethods * log(TDG height) * NumStateBits
	\item Linearize (time) =  order based on precondtions + (create graph + DFS + Dijkstra)*(as needed)
	\item Let the method size be V, and the number of edges be E
	\item Linearize = NumMethods * (NumStateBits + Method Size + Method Size + (orderings) + (Method Size)log(method Size))
\end{enumerate}
Therefore this algorithm is O(?) time?

 

\subsection{Solution preserving properties of Algorithm 1}
\textbf{Proposition 0.} \textit{Removes linearizations} \newline
\textit{Proof.}
This algorithm linearises all the methods to be totally ordered. Since sub-tasks inherit the orderings of their parents, it's impossible to preserve a solution that requires the interleaving of sub-tasks if their respective parents that are already ordered with respect to each other. This proves that the algorithm will always remove some linearizations, assuming the original domain was not already totally ordered.

Consider the simple problem:

F = \{a, b, c, g\}                                                           \newline
$N_p$ = \{$T_A, T_B, T_C$, G\}                                               \newline
$N_c$ = \{AB \}                                                               \newline
$\delta = \{ (T_A, A), (T_B, B), (T_C, C) \}$                                 \newline
M = $(AB,  \{ \{4,5\}, \{\}, \{(4,A), (5,B)\} \} )$                            \newline
$TN_Init$ = $\{ \{0,1,2\},  \{(0,2), (1,2)\}, \{(0,AB), (1,C), (2,G)\} \}$     \newline
	
	
$S_I$ = $<a>$	
	A = $<$pre a, del a, add c$>$          \newline
	C = $<$pre c, del c, add b$>$          \newline
	B = $<$pre b, del b, add g$>$          \newline
	G = $<$pre g, ,$>$                    \newline
	
	The initial task network enforces that G is the last action.
	To make G executable it needs the variable g, which only B can add.
	To make B executable it needs the variable b, which only C can add.
	To make C executable it needs the variable c, which only A can add.
	A is executable in the initial state.
	Therefore, the only solution is A C B G. This is impossible to achieve by linearizing methods, since either AB before C or C before AB, both of which exclude the solution. 
	
	This proves that the algorithm can remove all solutions, so Algorithm 1 is not complete.
	
	
\textbf{Proposition 1.} \textit{Soundness}  \newline
\textit{Proof.}
We do not modify the sub-tasks a method produces, just the ordering between them, so the set of plans from the totally ordered method is just a subset of the plans possible from the partially ordered one. Any solution to the linearized problem is then obviously a solution to the original problem.



\textbf{Proposition 2.} \textit{If Algorithm 1 didn't have to cycle-break, at least one solution is preserved} \newline
\textit{Proof.}
	Assume that there exists a solution in the PO domain. Using the same decomposition in the linearised domain, we can produce a linearization $(a_0, a_1, ..., a_n)$ of those actions in the PO solution. We then prove by induction over the sequence $(a_0, .. a_n)$ that it is executable.
 
	If $(a_0, ... a_n)$ is not executable, that means there exists some action $a_k,  0 < k < n$ that is not executable in the corresponding state.
	However $a_k$ must be executable in some linearization for $\{a_0, ..., a_n\}$, as we assumed it was a PO solution. So there must exist an action $a_i$, $0 < i < n$, that will adds A. Actions $a_0$ and $a_k$ must have a shared parent p in TDG. Then p has subtasks $t_0$ and $t_k$ that are parents of $a_0$ and $a_k$ respectively. 
	
	The linearization of this method would have drawn an ordering $(t_i, t_k)$ due to the way the algorithm defines $prec^{*}, add^{*}$ etc. We are assuming that all methods linearized without conflict, so $(t_i, t_k)$ should not be required. This safely enforces $(a_k, a_0)$ ordering in the final TO plan, meaning $a_0$ is not the first action in the resulting total order imposed by the algorithm. Therefore if $a_k$â€™s precondition could be met by any action $a_i$, $a_i$ would be ordered in front of it. 
	
	If $a_i$ does not exist then $a_k$ can never be executed for any linearization of $\{a_0, ...a_n\}$, contradicting the assumption that this was a PO solution. Since each action in the solution is executable, the entire sequence is executable linearization of actions produced by decomposition of initial task, i.e. the solution.
	

\textbf{2.1} \textit{Even if Algorithm 1 didn't have to cycle break, $\exists t. add^{*}(t) != add(t) \lor del^{*}(t) != del(t) $ }\newline
\textit{Proof.}
Suppose there exists an compound task t whose method 1 decomposes to an action a, with $add(a) = \{A\}$.
Assume there is another method which decomposes to action b, with $add(b) = \{B\}$
Therefore $prec^{*}(t) =\{A, B\} $ but both A and B, will not be applied in every execution of t.


\textbf{2.2} \textit{If Algorithm 1 didn't have to cycle break, there may be t such that $prec^{*}(t)$ } \newline
\textit{Proof.}
Assume some compound task t' decomposes into at least two sub-tasks, $\{t_1, t_2\}$. Suppose that $a \in F, a \in add(t_1)$ and
$a \in F, a \in prec(t_2)$. By the algorithm rules
\begin{itemize}			
		\item Add $\{ (t, t') |  (a \in add^{*}(t) \land a \in prec^{*}(t') \land t, t' \in tasks(m) ) \}$ to $NS_m$ 
		\item Add $\{ (t', t) |  (a \in add^{*}(t) \land a \in del^{*}(t') \land t, t' \in tasks(m) ) \}$ to $NS_m$ 
		\item Add $\{ (t', t) |  (a \in del^{*}(t) \land a \in prec^{*}(t') \land t, t' \in tasks(m) ) \}$ to $NS_m$ 
		\item Add $\{ (t, t') |  (a \in del^{*}(t) \land a \in add^{*}(t') \land t, t' \in tasks(m) ) \}$ to $NS_m$ 
\end{itemize}	
We know that $t_1$ is ordered before $t_2$. And by the previous proposition 2.1, we know that there does not exist a task $t_3$
such that $a \in del(t_3)$ Therefore, $t_2$ will always be executable regardless of what other tasks might do. However, $a \in prec^{*}(t')$ because $a \in prec^{*}(t_2)$. 
Therefore it's possible for preconditions to be erroneous, even if Algorithm 1 did not need to cycle break.

\textbf{Proposition 3.} \textit{When it does have to cycle break, it may preserve solutions} \newline
\textit{Proof.}
As per proposition 2.1 and 2.2, not all of the preconditions and/or effects are always needed. Suppose the initial task network consisted of 2 sub-tasks, $t_1, t_2$. If $t_2$ can decompose into an action $a_1$ such that $add(a_1) = A$, and an action $a_2$ such that $del(a_2) = A$,
and $t_1$ can decompose into an action $a_3$ with $prec(a_3) = A$
$t_2 = {add A, del A}$ and $t_1 = {prec A}$. The rules $\{(add A, prec A), (prec A, del A))\}$ of Algorthm 1
require orderings $\{(t_2, t_2), (t_2, t_1), (t_1, t_2)\}$, i.e.
Despite a cycle break being needed, this is obviously a solvable problem. Order $t_2$ before $t_1$ for this method.
Then when solving decompose $t_2$ to $a_1$ and $t_1$ to $a_3$ - this creates a solution. \newline \newline
\textbf{Proposition 4.} \textit{For some tasks it doesn't matter where they are executed} \newline
\textit{Proof.}
Assume that some pair of tasks $(t_1), (t_2)$ has no ordering between them. Assume also that $t_1 t_2$ is ok but $(t_2) (t_1)$ is not, i.e.
it matters where they are executed.  \newline  \newline
The required execution sequence implies that $t_2$ deletes some variable A $t_1$ relies on. But from the algorithm,
that would mean that an ordering $(t_1, t_2)$ was created, which contradicts the assumption that there is no ordering between them.
Therefore $(t_2)(t_1)$ must also be ok. \newline  \newline
Thus if some task t has no ordering from the algorithm, it can never matter where it is in relation to any other task.
The algorithm will not order it specifically when $(\forall t' \in m, \forall a \in prec(t). a \notin add(t') \land a \notin del(t'))
\land  (\forall t' \in m, \forall a \in add(t). a \notin prec(t') \land a \notin del(t')) 
\land  (\forall t' \in m, \forall a \in del(t). a \notin add(t') \land a \notin prec(t'))$
In other words, the variables that affect t, do not affect other tasks in that method.



\section{Algorithm 2} 
\begin{algorithm}[H]
	\KwData{$(F, T_P, T_C, \delta, M), \PreS, \AddS, \DelS$}
	\KwResult{$(F, T_P, T_C, \delta, M)$}
	
	\For {m $\in$ M}{
		$NS_m$  $\gets \emptyset$\;
		\For{a $\in$ F}{
			$NS_m$  = $\{ (t, t', 1) |  (a \in \AddS(t) \land a \in \PreS(t') \land t, t' \in tasks(m) ) \}$ $\cup$ $NS_m$  \;
			$NS_m$  = $\{ (t', t, 1) |  (a \in \AddS(t) \land a \in \DelS(t') \land t, t' \in tasks(m) ) \}$ $\cup$ $NS_m$  \;
			$NS_m$  =  $\{ (t', t, 1) |  (a \in \DelS(t) \land a \in \PreS(t') \land t, t' \in tasks(m) ) \}$ $\cup$ $NS_m$  \;
			$NS_m$  =  $\{ (t, t', 1) |  (a \in \DelS(t) \land a \in \AddS(t') \land t, t' \in tasks(m) ) \}$ $\cup$ $NS_m$  \;
		}{\tiny }
		directed graph G =(tasks(m),E)\;
		E $\gets \emptyset$ \;
		\For {ordering $\in \prec$} {
			E = E $\cup$ (ordering, 0)  
		} 
		\For {(t, t', w) $\in NS_m$}{
			E = E $\cup$ ((t,t'), w)  
		}       
		BackEdges $\gets$ DFS(G) \;
		\uIf{ $\vert BackEdges \vert > 0$ }
		{$\prec'$ = TopologicalSort(G)  \;
		$m' = (tasks(m), \prec', \alpha(t))$ \;
		$M' = m' \cup M'$ \;}
		\Else {$M' = m \cup M'$}
	}
	Return new domain = $(F, T_P, T_C, \delta, M')$
	\caption{Calculation of linearized methods}
\end{algorithm}

A possible option is to only linearize the methods which do not need cycle breaking.

\subsection{Solution preserving properties of Algorithm 2}
\textbf{Proposition 5.} \textit {Algorithm 2 is sound} \newline
\textit{Proof.} Same as Proposition 1


\textbf{Proposition 6.} \textit {Does Algorithm 2 preserve at least one solution? When?} \newline
\textit{Proof.} ?


\section{Algorithm 3}
The definitions in Conny's paper assumes that there are no negative preconditions.
Strict State-independent positive and negative effects:
 $$ \EffPlus := (\bigcap_{s \in E(c)} \bigcap_{s' \in R_s(c)} s')  \backslash  (\bigcap_{s \in E(c)} s)  $$
 $$ \EffMinus := \bigcap_{s \in E(c)} (F \backslash \bigcup_{s' \in R_s(c)}  s') $$
if $E(c) \neq \emptyset$ otherwise $eff^{+/-}_{*}(c) := undef$

Strict Possible state-independent effects:
 $$ \PossEffPlus := \bigcup_{s \in E(c)} (\bigcup_{s' \in R_s(c)} s'\backslash s)   $$
 $$ \PossEffMinus := \bigcup_{s \in E(c)} ((\bigcup_{s' \in R_s(c)} ( F \backslash s'))  \cap  s) $$


Relaxation: Define a new domain such that A' = ${ \{\emptyset, add, del \} |  \{\emptyset, add, del \} \in A }$.
Using this new domain, define the relaxed guaranteed and relaxed possible effects $\RelEffPlus$, 

$\RelEffMinus$, 

$\RelPossEffPlus$,

$ \RelPossEffMinus$ as before.

These effects will *definitely* happen, even if some methods cannot be linearised,
Unlike Algorithm 1, this set of preconditions and effects have no false candidates in them.
Unlike Algorithm 1, this set of preconditions and effects may be missing some State-independent positive and negative effects.


\begin{algorithm}[H]
	\KwData{$(F, T_P, T_C, \delta, M)$, \newline,
		$\RelEffPlus, \RelEffMinus, \RelPossEffPlus, \RelPossEffMinus$}
	\KwResult{$(F, T_P, T_C, \delta, M)$}
	
	\For {m $\in$ M}{
		$NS_m$  $\gets \emptyset$\;
		\For{a $\in$ F}{
			$NS_m$  = $\{ (t, t', 1) |  (a \in \AddS(t) \land a \in \PreS(t') \land t, t' \in tasks(m) ) \}$ $\cup$ $NS_m$  \;
			$NS_m$  = $\{ (t', t, 1) |  (a \in \AddS(t) \land a \in \DelS(t') \land t, t' \in tasks(m) ) \}$ $\cup$ $NS_m$  \;
			$NS_m$  =  $\{ (t', t, 1) |  (a \in \DelS(t) \land a \in \PreS(t') \land t, t' \in tasks(m) ) \}$ $\cup$ $NS_m$  \;
			$NS_m$  =  $\{ (t, t', 1) |  (a \in \DelS(t) \land a \in \AddS(t') \land t, t' \in tasks(m) ) \}$ $\cup$ $NS_m$  \;
			 Add edges between definite adds and definite deletes, weight =1  \;
			 Add edges between definite effects and possible preconditions, weight=2  \;
		  	Add edges between possible effects and possible preconditions, with weight=3   \;
		}
		directed graph G =(tasks(m),E)\;
		E $\gets \emptyset$ \;
		\For {ordering $\in \prec$} {
			E = E $\cup$ (ordering, 0)  
		} 
		\For {(t, t', w) $\in NS_m$}{
			E = E $\cup$ ((t,t'), w)  
		}       
	    BackEdges $\gets$ DFS(G) \;
		\For{e $\in$ BackEdges}{
			\uIf {weight(e) = 4}  {E = E $\setminus$ e} 
			%   Randomly pick an weight==3 edge in the (B, A) path and delete it. If it doesn't exist, search for weight=weight-1 to delete, etc,
			\Else {weight(e) = 3} { 
				A, B = e \; 
				Path = Dijkstra(B, A) \;
				randEdge = RANDOM( \{e $\vert$ weight(e) = 0 $\land$ e $\in Path$ \} ) \;
				E = E $\setminus$ randEdge
			} 
		}
		$\prec'$ = TopologicalSort(G) \;
		$m' = (tasks(m), \prec', \alpha(t))$ \;
		$M' = m' \cup M'$ \;
	}
	Return new domain = $(F, T_P, T_C, \delta, M')$
	\caption{Calculation of linearized methods}
\end{algorithm}



\subsection{Solution preserving properties of Algorithm 3}
\textbf{Proposition 7.} \textit {Algorithm 3 is sound} \newline
\textit{Proof.} Same as Proposition 1


\textbf{Proposition 8.} \textit {Does Algorithm 3 preserve at least one solution? When?} \newline
\textit{Proof.} ?
% If guaranteed precondtions/effects or those required by initial domain cause cycles in method, we know it actually needs interleaving and is not just a coincidence.

% \textbf{SOUND, NOT COMPLETE} \newline
% \textbf{PARTIAL LINEARIZATION: SOUND, COMPLETE?} \newline